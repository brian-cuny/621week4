---
output:
  pdf_document:
    df_print: kable
header-includes:
  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \fancyhead[L]{\textbf{Claim and Claim Cost Via Regression}}
  \fancyhead[R]{\thepage}
  \fancyfoot[C]{}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(knitr)
library(ROCR)
library(caret)
library(car)
library(mice)
library(lars)
insurance <- read_csv('C:\\Users\\Brian\\Desktop\\GradClasses\\Summer18\\621\\621week4\\insurance_training_data.csv') %>%
  select(-INDEX) %>% #don't need index
  mutate(TARGET_FLAG = factor(TARGET_FLAG),
         INCOME = as.numeric(str_replace_all(INCOME, '\\D', '')),
         PARENT1.CAT = factor(ifelse(PARENT1 == 'Yes', 1, 0)),
         HOME_VAL = as.numeric(str_replace_all(HOME_VAL, '\\D', '')),
         MSTATUS.CAT = factor(ifelse(MSTATUS == 'Yes', 1, 0)),
         MALE.CAT = factor(ifelse(SEX == 'M', 1, 0)),
         EDUCATION.CAT = factor(ifelse(EDUCATION == 'z_High School', 'High School', EDUCATION)),
         JOB.CAT = factor(ifelse(JOB == 'z_Blue Collar', 'Blue Collar', JOB)),
         PRIVATE.CAT = factor(ifelse(CAR_USE == 'Private', 1, 0)),
         BLUEBOOK = as.numeric(str_replace_all(BLUEBOOK, '\\D', '')),
         CAR_TYPE.CAT = factor(ifelse(CAR_TYPE == 'z_SUV', 'SUV', CAR_TYPE)),
         RED_CAR.CAT = factor(ifelse(RED_CAR == 'yes', 1, 0)),
         OLDCLAIM = as.numeric(str_replace_all(OLDCLAIM, '\\D', '')),
         REVOKED.CAT = factor(ifelse(REVOKED == 'Yes', 1, 0)),
         URBAN.CAT = factor(ifelse(URBANICITY == 'Highly Urban/ Urban', 1, 0))) %>%
  select(-SEX, -URBANICITY, -CAR_USE, -MSTATUS, -PARENT1, -JOB, -CAR_TYPE, -REVOKED, -RED_CAR, -EDUCATION)
```

# Abstract

I have been tasked with developing a logistic regression and a multiple linear regression that will determine (1) the likelyhood that a policy holder will make a claim on their car and (2) given that a claim is made, how much will it cost. Using both of these models we will be able to set rates for car insurance based on a number of predictors ranging from income, distance to work or number of kids at home. Their are 8161 observations in the training set with 23 predictors. There are 2 response variables, the binary value indicating whether a claim was made and a numeric value indicating the cost of said cliam.

I will develop three logistic regression models, explore each, and ultimately select the strongest model to use on the evaluation set.
I will then develop two multiple linear regressions, explore both, and select the strongest model to use on the evaluation set.

I will begin by exploring the data set as a whole and then each individual predictor.

# Data Exploration

Upon reading in the provided data, I made a small number of changes to the names of the columns to aid in my work. Categorical variables have a '.CAT' added to the end and variables with responses such as 'yes/no' have been changed to '1/0' with a more identifying value. For example 'SEX' became 'MALE.CAT'. A number of responses also had odd artifcats that needed to be addressed. 

After cleaning up the data, their values were explored. Initial inspection of the data shows a small number of missing values in AGE, YOJ, INCOME, HOME_VAL, CAR_AGE and JOB.CAT. Considering the small number of missing values, it is reasonble to impute them. The below plot shows the distribution of the missing values.

```{r, cache=TRUE}
insurance %>%
  map_dbl(~sum(is.na(.))/nrow(insurance)) %>%
  kable()
```

```{r, cache=TRUE, message=FALSE, warning=FALSE}
VIM::aggr(insurance[, c(-1, -2)], col=c('navyblue', 'yellow'),
          numbers=TRUE, sortVars=TRUE, 
          labels=names(insurance[, c(-1, -2)]), cex.axis=.7,
          gap=3, ylab=c('Missing Data', 'Pattern'), combined=TRUE)
```

I will use the mice library to impute the data. Once complete I will create a new data frame that has the imputed values.

```{r, cache=TRUE, message=FALSE, warning=FALSE}
set.seed(123)
imputed.data <- mice::mice(insurance[, c(-1, -2)], m=5, maxit=50, method='pmm', 
                           seed=500, printFlag=FALSE)
insurance.complete <- cbind(insurance[, c(1, 2)], complete(imputed.data, 1))
```

I will partition the data into a training set (80%) and a testing set (20%), seperate from the evaluation set that I will use on the selected model. After partitioning the data, I will use 10-fold cross-validation in training my models. About 25% of the customers in the full training set made a claim. This will be reflected in the partition.

```{r, cache=TRUE}
set.seed(1)
part <- caret::createDataPartition(insurance.complete$TARGET_FLAG, p=0.8, list=FALSE)
log.training <- insurance.complete[, -2] %>%
  filter(row_number() %in% part)
log.testing <- insurance.complete[, -2] %>%
  filter(!row_number() %in% part)

set.seed(1)
lm.insurance.complete <- insurance.complete %>%
  filter(TARGET_AMT != 0)
part <- caret::createDataPartition(lm.insurance.complete$TARGET_AMT, p=0.8, list=FALSE)
lin.training <- lm.insurance.complete[, -1] %>%
  filter(row_number() %in% part)
lin.testing <- lm.insurance.complete[, -1] %>%
  filter(!row_number() %in% part)
```

With all the values imputed, I am ready to start my initial exploration of the predictors. I created three functions to help with this analysis.

```{r, cache=TRUE, echo=FALSE}
Predictor.Discrete.Disp <- function(x){
  require(gridExtra)
  plot.1 <- ggplot(log.training, aes_string(x)) +
    geom_bar() +
    labs(y = 'Count',
    title = 'Predictor Distribution') +
    theme_bw() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0, 0.05, 0.05))
  
  plot.2 <- 
    log.training %>%
    mutate(TARGET_FLAG = factor(TARGET_FLAG)) %>%
    ggplot(aes_string('TARGET_FLAG', x)) +
    geom_boxplot() +
    geom_jitter(alpha=0.2) +
    theme_bw() +
    labs(x='',
         y='',
         title='Variance by Target Value') +
    scale_x_discrete(labels = c('No Claim', 'Claim')) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank())
  
  plot.3 <-   ggplot(lin.training, aes_string(x=x)) +
    geom_point(aes(y=log(TARGET_AMT))) +
    geom_smooth(aes(y=log(TARGET_AMT)), method='lm') +
    theme_bw() +
    labs(x = 'Predictor',
         y = 'Log Response')
  grid.arrange(plot.1, plot.2, plot.3, ncol=2)
}

Predictor.Disp <- function(x){
  require(gridExtra)
  plot.1 <- ggplot(log.training, aes_string(x)) +
    geom_density() +
    labs(y = 'Density',
    title = 'Predictor Distribution') +
    theme_bw() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0, 0.05, 0.05))
  
  plot.2 <- 
    log.training %>%
    mutate(TARGET_FLAG = factor(TARGET_FLAG)) %>%
    ggplot(aes_string('TARGET_FLAG', x)) +
    geom_boxplot() +
    geom_point(alpha=0.2) +
    theme_bw() +
    labs(x='',
         y='',
         title='Variance by Target Value') +
    scale_x_discrete(labels = c('No Claim', 'Claim')) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank())
  
  plot.3 <-   ggplot(lin.training, aes_string(x=x)) +
    geom_point(aes(y=log(TARGET_AMT))) +
    geom_smooth(aes(y=log(TARGET_AMT)), method='lm') +
    theme_bw() +
    labs(x = 'Predictor',
         y = 'Log Response')
  grid.arrange(plot.1, plot.2, plot.3, ncol=2)
}
```

##KIDSDRIV

**The number of kids that drive the car on the policy**

This predictor is discrete with values ranging only from 0 to 4. It is heavily skewed with most cars having 0 kid drivers. Examining the table of values, it appears
that having any number of kid driver's results in a higher likelyhood of making a claim.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Discrete.Disp('KIDSDRIV')
```

##AGE

**Age of the Driver**

Age has a nice, normal distribution centered around 45. The distribution based on whether a claim is made or not is nearly identical. This leads me to believe that age will not be helpful in determining the likelyhook of making a claim.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('AGE')
```

##YOJ

**Years On Job**

This predictor is nearly normal other than people who are currently unemployed. The distribution when seperated by predictor shows no meanginful difference. It is unlikely that we will use this variable.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('YOJ')
```

##INCOME

**Yearly Income**

Income is, just like in the general population, heavily skewed. This is represented in the boxplot as well as there are numerous upper outliers in both cases. The correlation between YOJ and INCOME is not as large as one might imagine.

There are outliers on INCOME that may need to be addressed when refering to the TARGET_AMT.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('INCOME')
```

##HOME_VAL

**Value of Home**

Usefullness of this predictor may be dented by the large number of people who do not own a home. It may be worth considering seperating this into a categorical variable represnting whether or not someone owns a home. The value of the home may be captured by INCOME.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('HOME_VAL')
```

##TRAVTIME

**Distance to Work**

The distance travelled to work is fairly normal and the boxplots show only a subtle increase in the likelyhood of making a claim.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('TRAVTIME')
```

##BLUEBOOK

**Value of Vehicle**

The boxplot indicates that those making a claim have a car that is lower in value. Could this be that more expensive cars are driven more carefully due to their cost or is this a confounding variable that once again measures INCOME?

BLUEBOOK is the only variable that appears to show any ability to capture the value of the response variable TARGET_AMT.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('BLUEBOOK')
```

##TIF

**Length of Stay with Company**

The density plot of this predictor indicates that it could be considered as discrete. There appears to be a significant decrease in the likelyhood of making a claim the longer the person has been with the company. That is, safe drivers tend to stay safe.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('TIF')
```

##OLDCLAIM

**Claims cost made in the Past 5 Years**

Heavily, heavily skewed predictor. Most people do not make claims.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('OLDCLAIM')
```

##CLM_FREQ

**Number of claims made in the Past 5 Years**

This predictor appears to be highly significant against people who have made a past claim. That is, people who have made a claim in the past 5 years are very likely to make another claim.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Discrete.Disp('CLM_FREQ')
```

##MVR_PTS

**Motor Vehicle Record Points**

This predictor can be seen as a proxy for how safe a driver someone is. Receiving points on a license indicates that the driver has likely been caught speeding, tailgating or other dangerous driving activities. The boxplot indicates that this variable is likely to be highly significant.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('MVR_PTS')
```

##CAR_AGE

**Age of the Vehicle**

This predictor is bimodal, indicating that most cars are either brand new or quite old. There is one data point that is clearly mislabeled as it indicates the car is -3 years old. This will be corrected to 0. There is no indication whether 0, 3 or some other number is the correct choice but considering it is one value amongst many 10s of thousands it is unlikely to have any meaningful effect on the regression.

```{r, message=FALSE, echo=FALSE, cache=TRUE, fig.height=4, fig.width=10, fig.align='center'}
Predictor.Disp('CAR_AGE')
```

##MALE.CAT

**Categorical 0 is Female, 1 if Male**

This variable was derived from SEX, just to make the variable's meaning more clear. There appears to be no meanginful difference when considering the gender of the driver.

```{r, message=FALSE, echo=FALSE, cache=TRUE}
table(MALE = log.training$MALE.CAT, TARGET=log.training$TARGET_FLAG)
```

##EDUCATION.CAT

**Categorical representing max education level**

This variable will need to be monitored as it may be correlated with INCOME or YOJ.

```{r, message=FALSE, echo=FALSE, cache=TRUE}
table(EDUCATION = log.training$EDUCATION.CAT, TARGET = log.training$TARGET_FLAG)
```

##PRIVATE.CAT

**Categorical 0 is commerical, 1 if private**

This variable was derived from CAR_USE, just to make the variable's meaning more clear.

```{r, message=FALSE, echo=FALSE, cache=TRUE}
table(PRIVATE = log.training$PRIVATE.CAT, TARGET = log.training$TARGET_FLAG)
```

##CAR_TYPE.CAT

**Categorical representing the car's type**

Certain cars are popular with more aggressive or less safe drivers. This may assist in identifying the likelyhood of making a claim.

```{r, message=FALSE, echo=FALSE, cache=TRUE}
table(CAR_TYPE = log.training$CAR_TYPE.CAT, TARGET = log.training$TARGET_FLAG)
```

##RED_CAR.CAT

**Categorical 0 if not Red, 1 if Red**

Urban legend states that red cars stand out to police officers and are thus more likely to get pulled over or find themselves in perilous situations.

```{r, message=FALSE, echo=FALSE, cache=TRUE}
table(RED_CAR = log.training$RED_CAR.CAT, TARGET = log.training$TARGET_FLAG)
```

##REVOKED.CAT##

**Categorical 0 is license not revoked, 1 is revoked**

The table's distribution paints a bleak picture that customers who have previously lost their license are likely to be in future accidents.

```{r, message=FALSE, echo=FALSE, cache=TRUE}
table(REVOKED = log.training$REVOKED.CAT, TARGET = log.training$TARGET_FLAG)
```

##URBAN.CAT##

**Categorical 0 is not urban home/work area, 1 is urban home/work area**

This variable can be seen as a proxy for whether the driver frequently uses highways. Urban driving is more likely to result in making a claim, but highway claims are more likely to be expensive. (Collisions at 25mph are obviously less damaging than at 65mph).

```{r, message=FALSE, echo=FALSE, cache=TRUE}
table(URBAN = log.training$URBAN.CAT, TARGET = log.training$TARGET_FLAG)
```

#Logistic Regression

##Model 1

For the first model, I will consider only the categorical variables. This model has the advantage of being easily interpretable and the easiest to calculate for future customers. 

I began by adding in all the categorical predictors and then examining which, if any, should be removed from the regression. I considered 5 different methods for model selection. [SEE APPENDIX]

**drop1** suggested keeping all the predictors

**AIC** suggested dropping RED_CAR.CAT

**BIC** suggested dropping RED_CAR.CAT

**lasso** suggested dropping RED_CAR.CAT along with specific values from JOB.CAT and CAR_TYPE.CAT, which is not recommended.

**manual selection** suggested RED_CAR.CAT only

Based on the above 5 methods, the final version of model 1 will only drop RED_CAR.CAT

```{r, cache=TRUE}
ctrl <- trainControl(method='repeatedcv', number=10, savePredictions=TRUE)
model.1 <- train(TARGET_FLAG ~ PARENT1.CAT + MSTATUS.CAT + MALE.CAT + EDUCATION.CAT + 
                   JOB.CAT + PRIVATE.CAT + CAR_TYPE.CAT + REVOKED.CAT + URBAN.CAT, 
                 data=log.training, method='glm', family='binomial',
                 trControl=ctrl, tuneLength=5)
summary(model.1)
```

##Model 2

For the second model, I will begin by adding in every single predictor, running the regression and then iteratively remove terms based on my analysis. [SEE APPENDIX]

**drop1** suggested keeping all the predictors

**AIC** suggested dropping AGE, YOJ, CAR_AGE, MALE.CAT, RED_CAR.CAT

**BIC** suggested dropping AGE, YOJ, CAR_AGE, MALE.CAT, JOB.CAT, RED_CAR.CAT

**lasso** suggested dropping nothing

**manual selection** suggested dropping RED_CAR.CAT, AGE, YOJ, CAR_AGE, MALE.CAT, HOMEKIDS

Based on the above 5 methods, the final verison of model 2 will drop AGE, YOJ, CAR_AGE, MALE.CAT and RED_CAR.CAT

```{r, cache=TRUE}
ctrl <- trainControl(method='repeatedcv', number=10, savePredictions=TRUE)
model.2 <- train(TARGET_FLAG ~ . -AGE -YOJ -CAR_AGE -MALE.CAT -RED_CAR.CAT, 
                 data=log.training, method='glm', family='binomial',
                 trControl=ctrl, tuneLength=5)
summary(model.2)
```

##Model 3

Examining the diagnostic plots for model 2 indicates that there are a number of predictors that may require a quadratic term. For the final model I will add in all these potential quadratic terms and select a model from there.

**drop1** suggested keeping all the predictors

**AIC** suggested dropping AGE, HOMEKIDS^2, YOJ, MVR_PTS, CAR_AGE, MALE.CAT, RED_CAR

**BIC** suggested dropping KIDSDRIV^2, AGE, HOMEKIDS, HOMEKIDS^2, YOJ, CLM_FREQ^2, MVR_PTS, CAR_AGE, MALE.CAT, JOB.CAT, RED_CAR.CAT

I will be more aggresive with this final model and select the BIC suggestion.

```{r, cache=TRUE}
log.training <- log.training %>%
  mutate(MVR_PTS2 = MVR_PTS*MVR_PTS)
ctrl <- trainControl(method='repeatedcv', number=10, savePredictions=TRUE)
model.3 <- train(TARGET_FLAG ~ . -AGE -HOMEKIDS -YOJ -MVR_PTS -CAR_AGE -MALE.CAT 
                 -JOB.CAT -RED_CAR.CAT,
                 data=log.training, method='glm', family='binomial',
                 trControl=ctrl, tuneLength=5)
summary(model.3)
```

##Logistic Model Selection

$R^2$ does not exist for logistic regression in the traditional sense. However, there are a number of so called pseudo $R^2$ terms that can be analized. This is a good starting point for identifying the relative strength of each model.

```{r}
model.1.diag <- glm(TARGET_FLAG ~ PARENT1.CAT + MSTATUS.CAT + MALE.CAT + EDUCATION.CAT + 
                      JOB.CAT + PRIVATE.CAT + CAR_TYPE.CAT + 
                   REVOKED.CAT + URBAN.CAT, data=log.training, family=binomial)
model.2.diag <- glm(TARGET_FLAG ~ . -AGE -YOJ -CAR_AGE -MALE.CAT -RED_CAR.CAT, 
                    data=log.training, family=binomial)
model.3.diag <- glm(TARGET_FLAG ~ . -AGE -HOMEKIDS -YOJ - MVR_PTS -CAR_AGE -MALE.CAT 
                    -JOB.CAT -RED_CAR.CAT +I(MVR_PTS^2), 
                    data=log.training, family=binomial)
data_frame(name=names(pscl::pR2(model.1.diag)), value=pscl::pR2(model.1.diag)) %>% 
  spread(1, 2) %>%
  kable()
data_frame(name=names(pscl::pR2(model.2.diag)), value=pscl::pR2(model.2.diag)) %>% 
  spread(1, 2) %>%
  kable()
data_frame(name=names(pscl::pR2(model.3.diag)), value=pscl::pR2(model.3.diag)) %>% 
  spread(1, 2) %>%
  kable()
```

The first model is the weakest while the second and third are close in their psuedo-$R^2$. Next, we will examine the ROC curve to determine a good cutoff point for categorization against the testing data. All three models appear to have between 0.5 to 0.4 as a good compromise.

```{r, cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
ROCRPred <- prediction(predict(model.1.diag, type='response'), log.training$TARGET_FLAG)
ROCRPref <- performance(ROCRPred, 'tpr', 'fpr')
plot(ROCRPref, colorize=TRUE, print.cutoffs.at = seq(0.1, by=0.1))
```

```{r, cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
ROCRPred <- prediction(predict(model.2.diag, type='response'), log.training$TARGET_FLAG)
ROCRPref <- performance(ROCRPred, 'tpr', 'fpr')
plot(ROCRPref, colorize=TRUE, print.cutoffs.at = seq(0.1, by=0.1))
```

```{r, cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
ROCRPred <- prediction(predict(model.3.diag, type='response'), log.training$TARGET_FLAG)
ROCRPref <- performance(ROCRPred, 'tpr', 'fpr')
plot(ROCRPref, colorize=TRUE, print.cutoffs.at = seq(0.1, by=0.1))
```

Finally, I will create a confusion matrix for each model against the testing data.

```{r}
predictions <- ifelse(predict(model.1, newdata=log.testing, type='prob')[2] < 0.4, 0, 1)
caret::confusionMatrix(table(predicted=predictions, actual = log.testing$TARGET_FLAG))
```

```{r}
predictions <- ifelse(predict(model.2, newdata=log.testing, type='prob')[2] < 0.4, 0, 1)
caret::confusionMatrix(table(predicted=predictions, actual = log.testing$TARGET_FLAG))
```

```{r}
log.testing <- log.testing %>%
  mutate(MVR_PTS2 = MVR_PTS*MVR_PTS)
predictions <- ifelse(predict(model.3, newdata=log.testing, type='prob')[2] < 0.4, 0, 1)
caret::confusionMatrix(table(predicted=predictions, actual = log.testing$TARGET_FLAG))
```

Based on all the available diagnostics, I will select **model 2**. It has the highest pseudo-$R^2$, it performed the strongest on the testing set and has the lowest AIC.

#LINEAR REGRESSION

## Model 4

The initial predictor exploration does not bode well for the linear regression. None of the predictors appeared to show any real ability to predict the response variable. I will begin by include all the predictors compared against the log transformed TARGET_AMT. The log transformation was deemed necessary by viewing the distribution of TARGET_AMT and it's relationship to each of the predictors.

```{r, cache=TRUE}
ln.model.1 <- lm(log(TARGET_AMT) ~ ., data=lin.training)
summary(ln.model.1)
```

The summary shows that only BLUEBOOK appears to have any predictive ability. This was the initial conclusion of the exploratory analysis as well. [SEE APPENDIX]

## Model 5

This model was selected by using model shrinkage. [SEE APPENDIX]

```{r, cache=TRUE}
ln.model.2 <- lm(log(TARGET_AMT) ~ BLUEBOOK + CLM_FREQ + MVR_PTS + MSTATUS.CAT + 
                   RED_CAR.CAT, data=lin.training)
summary(ln.model.2)

```

Despite including only the most important predictors, 3 of them are still not statistically significant. The below anova test demonstrates that there was no apparent loss of predicive ability by dropping the other terms.

```{r}
anova(ln.model.1, ln.model.2)
```

##Model 6

I noticed during the exploratory analysis that there were a number of upper and lower outliers that may be skewing the results of the regression. I will perform a robusted regression for this model and further explore BLUEBOOK, the most significant of the predictors in more depth. [SEE APPENDIX]

```{r, cache=TRUE}
ln.model.3 <- MASS::rlm(log(TARGET_AMT) ~ poly(BLUEBOOK, 2), data=lin.training)
summary(ln.model.3)
```

```{r}
anova(ln.model.2, ln.model.3)
```

There does not appear to be any significant difference in the predictive ability of all three models. 

##MODEL SELECTION

None of the three models have demonstrated a strong predictive ability and all have a small, similar $R^2$. It appears that we do not have the necessary predictors to conclusively determine the response. To help determine which model to select, I will test their predictions against the testing set.

```{r}
pred.1 <- exp(predict(ln.model.1, newdata=lin.testing, interval='confidence'))
sum(ifelse(pred.1[, 2] < lin.testing[, 1] & pred.1[, 3] > lin.testing[, 1], 1, 0))

pred.2 <- exp(predict(ln.model.2, newdata=lin.testing, interval='confidence'))
sum(ifelse(pred.2[, 2] < lin.testing[, 1] & pred.2[, 3] > lin.testing[, 1], 1, 0))

pred.3 <- exp(predict(ln.model.3, newdata=lin.testing, interval='confidence'))
sum(ifelse(pred.3[, 2] < lin.testing[, 1] & pred.3[, 3] > lin.testing[, 1], 1, 0))
```

While none of the models did a particularly strong job of predicting TARGET_AMT, the first model did by far the best. This is the model that included over a dozen non-significant predictors. This has me concerned about overfitting. This is especially due to the fact that an anova test determined no significant difference in the first and second models. For that reason, I have decied to select **model 5**.

#CONCLUSION

As required, the predictions for the logistic regression and linear regression have been written out to csv files. However, in the end neither model turned out to be particularly powerful. The logistic regression was able to get near an 80% accurate prediction rate, however considering that a 75% prediction rate could be obtained by simply guessing that a claim would NOT be made, 80% is not as strong as it appears. In the same veign, it appears that essentially none of our predictors is useful for identifying the cost of a claim other than the current value of the car. The $R^2$ on the linear regression was particularly poor at about 0.02. No modeling techniques were able to overcome this barrier.

In a way, this exploration has only reaffirmed the need for insurance. Car collisions are somewhat rare and can happen to anyone at any time. The likelyhood and cost of the incident can vary widely. As a result of not being able to predict accurately when such incidents will occur, we require that all drivers protect themselves with insurance.

```{r cache=TRUE, message=FALSE}
evaluation.data <- read_csv('C:\\Users\\Brian\\Desktop\\GradClasses\\Summer18\\621\\621week4\\insurance-evaluation-data.csv') %>%
  select(-INDEX) %>% #don't need index
  mutate(TARGET_FLAG = factor(TARGET_FLAG),
         INCOME = as.numeric(str_replace_all(INCOME, '\\D', '')),
         PARENT1.CAT = factor(ifelse(PARENT1 == 'Yes', 1, 0)),
         HOME_VAL = as.numeric(str_replace_all(HOME_VAL, '\\D', '')),
         MSTATUS.CAT = factor(ifelse(MSTATUS == 'Yes', 1, 0)),
         MALE.CAT = factor(ifelse(SEX == 'M', 1, 0)),
         EDUCATION.CAT = factor(ifelse(EDUCATION == 'z_High School', 'High School', EDUCATION)),
         JOB.CAT = factor(ifelse(JOB == 'z_Blue Collar', 'Blue Collar', JOB)),
         PRIVATE.CAT = factor(ifelse(CAR_USE == 'Private', 1, 0)),
         BLUEBOOK = as.numeric(str_replace_all(BLUEBOOK, '\\D', '')),
         CAR_TYPE.CAT = factor(ifelse(CAR_TYPE == 'z_SUV', 'SUV', CAR_TYPE)),
         RED_CAR.CAT = factor(ifelse(RED_CAR == 'yes', 1, 0)),
         OLDCLAIM = as.numeric(str_replace_all(OLDCLAIM, '\\D', '')),
         REVOKED.CAT = factor(ifelse(REVOKED == 'Yes', 1, 0)),
         URBAN.CAT = factor(ifelse(URBANICITY == 'Highly Urban/ Urban', 1, 0)),
         MVR_PTS2 = MVR_PTS*MVR_PTS) %>%
  select(-SEX, -URBANICITY, -CAR_USE, -MSTATUS, -PARENT1, -JOB, -CAR_TYPE, -REVOKED, -RED_CAR, -EDUCATION)

write_csv(as_data_frame(predict(model.2.diag, newdata=evaluation.data, type='response')), 'logistic_regression_output.csv')
write_csv(as_data_frame(predict(ln.model.2, newdata=evaluation.data, interval='confidence')), 'linear_regression_output.csv')
```

#APPENDIX

##LOGISTIC REGRESSION

##Model 1 Selection

```{r, message=FALSE, cache=TRUE, fig.height=4, fig.width=8, fig.align='center'}
model.1.full <- glm(TARGET_FLAG ~ PARENT1.CAT + MSTATUS.CAT + MALE.CAT + EDUCATION.CAT + 
                      JOB.CAT + PRIVATE.CAT + CAR_TYPE.CAT + 
                      RED_CAR.CAT + REVOKED.CAT + URBAN.CAT, family=binomial, data=log.training)
drop1(model.1.full)
MASS::stepAIC(model.1.full, trace=0)
MASS::stepAIC(model.1.full, k=log(nrow(log.training)), trace=0)

set.seed(123)
model.1.lasso <- lars(model.matrix(~ PARENT1.CAT + MSTATUS.CAT + MALE.CAT + EDUCATION.CAT 
                                   + JOB.CAT + PRIVATE.CAT + CAR_TYPE.CAT + 
                                     RED_CAR.CAT + REVOKED.CAT + URBAN.CAT, log.training), 
                      as.numeric(log.training$TARGET_FLAG))
cvlmod <- cv.lars(model.matrix(~ PARENT1.CAT + MSTATUS.CAT + MALE.CAT + EDUCATION.CAT + 
                                 JOB.CAT + PRIVATE.CAT + CAR_TYPE.CAT + 
                                 RED_CAR.CAT + REVOKED.CAT + URBAN.CAT, log.training), 
                  as.numeric(log.training$TARGET_FLAG))
predict(model.1.lasso, s=0.9494949, type='coef', mode='fraction')$coef
```

##Model 2 Selection

```{r, message=FALSE, cache=TRUE, fig.height=4, fig.width=8, fig.align='center'}
model.2.full <- glm(TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + 
                      HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + CLM_FREQ + 
                      MVR_PTS + CAR_AGE + PARENT1.CAT + MSTATUS.CAT + MALE.CAT + 
                      EDUCATION.CAT + JOB.CAT + PRIVATE.CAT + CAR_TYPE.CAT + 
                      RED_CAR.CAT + REVOKED.CAT + URBAN.CAT, data=log.training, family=binomial)
drop1(model.2.full) 
MASS::stepAIC(model.2.full, trace=0) 
MASS::stepAIC(model.2.full, k=log(nrow(log.training)), trace=0) 

set.seed(123)
model.2.lasso <- lars(model.matrix(~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + HOME_VAL + 
                                     TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + CLM_FREQ +
                                     MVR_PTS + CAR_AGE + PARENT1.CAT + MSTATUS.CAT + 
                                     MALE.CAT + EDUCATION.CAT + JOB.CAT +
                                     PRIVATE.CAT + CAR_TYPE.CAT + RED_CAR.CAT + REVOKED.CAT + 
                                     URBAN.CAT, log.training), as.numeric(log.training$TARGET_FLAG))
cvlmod <- cv.lars(model.matrix(~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + HOME_VAL + 
                                 TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + CLM_FREQ +
                                 MVR_PTS + CAR_AGE + PARENT1.CAT + MSTATUS.CAT + MALE.CAT + 
                                 EDUCATION.CAT + JOB.CAT +
                                 PRIVATE.CAT + CAR_TYPE.CAT + RED_CAR.CAT + REVOKED.CAT + 
                                 URBAN.CAT, log.training), as.numeric(log.training$TARGET_FLAG))
cvlmod$index[which.min(cvlmod$cv)] 
predict(model.2.lasso, s=0.9292929, type='coef', mode='fraction')$coef
```

##Model 3 Selection

```{r, message=FALSE, cache=TRUE, fig.height=4, fig.width=8, fig.align='center'}
model.3.full <- glm(TARGET_FLAG ~ KIDSDRIV + I(KIDSDRIV ^ 2) + AGE + HOMEKIDS + 
                      I(HOMEKIDS ^ 2) + YOJ + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + 
                      TIF + OLDCLAIM + CLM_FREQ + I(CLM_FREQ ^ 2) + MVR_PTS + I(MVR_PTS ^ 2) + 
                      CAR_AGE + PARENT1.CAT + MSTATUS.CAT + MALE.CAT + EDUCATION.CAT + JOB.CAT +
                      PRIVATE.CAT + CAR_TYPE.CAT + RED_CAR.CAT + REVOKED.CAT + 
                      URBAN.CAT, data=log.training, family=binomial)
drop1(model.3.full) 
MASS::stepAIC(model.3.full, trace=0) 
MASS::stepAIC(model.3.full, k=log(nrow(log.training)), trace=0)
```

##Model 4 Selection

Model 4 shows no apparent outliers or bad leverage points. This is a valid regression.

```{r, message=FALSE, cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
par(mfrow=c(2,2))
plot(ln.model.1)
```

##Model 5 Selection

Model 5 was simplified using stepAIC

```{r, cache=TRUE}
MASS::stepAIC(ln.model.1, trace=0)
```

##DIAGNOSTICS

##LOGISTIC REGRESSION

Multi-collinearity is not an issue for any of the three models.

```{r, cache=TRUE}
car::vif(model.2.diag)
```

Finally, I have the mmps plots to demonstrate a good fit.

```{r, message=FALSE, warning=FALSE, cache=TRUE, fig.height=4, fig.width=8, fig.align='center'}
car::marginalModelPlots(model.2.diag)
```

##LINEAR REGRESSION

Examination of the predictors against the response variable TARGET_AMT reveal that TARGET_AMT should be log transformed.

```{r, cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
lin.training %>%
  select_if(is.numeric) %>%
  gather(key='Predictor', value='Value', 2:13) %>%
  ggplot(aes(Value, log(TARGET_AMT))) +
  geom_point() +
  geom_smooth(method='lm') +
  facet_wrap(~Predictor, scale='free_x')
```

Multi-collinearity is not an issue for any of the three models.

```{r, cache=TRUE}
car::vif(ln.model.2)
```

A plot of diagnostics and mmps demonstrate that this is a valid regression. There are a number of lower outliers present due to the extremely small size of the claim.

```{r, message=FALSE, warning=FALSE, cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
par(mfrow=c(2,2))
plot(ln.model.2)
car::mmps(ln.model.2)
```




